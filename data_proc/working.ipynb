{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parse as p\n",
    "import gensim as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\metrics.py:4: DeprecationWarning: sklearn.metrics.metrics is deprecated and will be removed in 0.18. Please import from sklearn.metrics\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "l_interested = p.get_entity_list(p.plib.interested_libs)\n",
    "#l_code = list(map(get_code, l_interested))\n",
    "l_doc = list(map(p.get_doc, l_interested))\n",
    "#l_doc = list(map(pdoc.extract, l_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = l_doc\n",
    "texts = p.simple_process(documents=documents, stoplist=p.read_stoplist(p.FNAME_STOPLIST))\n",
    "texts = p.remove_infrequent(texts, n_times=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347 3164 2039 929 988 5051 1810 1432 3765 517\n"
     ]
    }
   ],
   "source": [
    "id2word = g.corpora.Dictionary(texts)\n",
    "#id2word.save('/tmp/deerwester.dict') # store the id2word, for future reference\n",
    "print(*list(id2word)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 1), (3, 4), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1)] [(1, 2), (3, 3), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 2)] [(15, 1), (21, 2), (22, 3), (23, 2), (24, 2), (25, 2), (26, 1), (27, 2), (28, 4), (29, 1), (30, 1), (31, 1), (32, 2), (33, 3), (34, 1), (35, 1), (36, 2)] [(21, 1), (24, 1), (33, 2), (37, 1), (38, 1), (39, 2), (40, 2)] [(15, 2), (29, 1), (30, 1), (31, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1)] [(3, 1), (5, 1), (9, 1), (17, 1), (23, 3), (39, 3), (44, 1), (45, 1), (46, 1), (47, 3), (48, 1), (49, 4), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 4), (56, 1), (57, 3), (58, 1), (59, 2), (60, 1), (61, 1), (62, 1), (63, 1)] [(3, 1), (39, 2), (46, 1), (49, 2), (53, 1), (55, 2), (64, 1), (65, 1), (66, 3), (67, 2)] [(0, 1), (3, 1), (5, 1), (9, 1), (23, 3), (39, 3), (44, 1), (47, 2), (48, 1), (51, 3), (52, 1), (53, 1), (54, 3), (55, 4), (58, 1), (59, 1), (60, 1), (64, 1), (68, 1), (69, 4), (70, 2), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 3), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1)] [(0, 1), (3, 2), (5, 1), (17, 1), (23, 4), (39, 3), (46, 1), (53, 1), (55, 3), (85, 1), (86, 4), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 2), (93, 3)] [(3, 1), (4, 1), (6, 2), (40, 1), (94, 2), (95, 1), (96, 1), (97, 2), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 2), (105, 2), (106, 1), (107, 1), (108, 1), (109, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "#g.corpora.corpusCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use\n",
    "print(*list(corpus)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load id->word mapping (the id2word), one of the results of step 2 above\n",
    "#id2word = g.g.corpora.id2word.load_from_text('wiki_en_wordids.txt')\n",
    "# load corpus iterator\n",
    "#corpus = g.corpora.MmCorpus('/tmp/deerwester.mm')\n",
    "#corpus = g.g.corpora.MmCorpus(bz2.BZ2File('wiki_en_tfidf.mm.bz2')) # use this if you compressed the TFIDF output (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n",
    "#lda = g.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, \n",
    "#                                 update_every=1, chunksize=10000, passes=5)\n",
    "## print the most contributing words for n_topic topics\n",
    "#print(*lda.print_topics(n_topics), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.576*\"arrai\" + 0.286*\"option\" + 0.253*\"np\" + 0.174*\"default\" + 0.174*\"shape\" + 0.166*\"paramet\" + 0.149*\"axi\" + 0.149*\"int\" + 0.138*\"function\" + 0.138*\"matrix\"')\n",
      "(1, '-0.510*\"arrai\" + 0.294*\"option\" + 0.286*\"function\" + 0.204*\"method\" + -0.167*\"np\" + 0.162*\"optim\" + 0.161*\"matrix\" + -0.158*\"axi\" + 0.138*\"algorithm\" + 0.137*\"iter\"')\n",
      "(2, '0.390*\"matrix\" + 0.325*\"shape\" + -0.223*\"np\" + -0.221*\"axi\" + -0.216*\"option\" + 0.208*\"n_sampl\" + 0.180*\"fit\" + 0.162*\"math\" + 0.146*\"coeffici\" + -0.144*\"plt\"')\n",
      "(3, '0.354*\"plt\" + 0.310*\"window\" + 0.208*\"filter\" + -0.200*\"arrai\" + 0.190*\"frequenc\" + 0.170*\"math\" + 0.166*\"sampl\" + -0.160*\"method\" + -0.157*\"optim\" + 0.155*\"signal\"')\n",
      "(4, '0.342*\"math\" + 0.283*\"distanc\" + -0.255*\"fals\" + -0.248*\"true\" + -0.216*\"data\" + 0.208*\"matrix\" + 0.193*\"vector\" + -0.182*\"shape\" + 0.169*\"comput\" + 0.165*\"np\"')\n",
      "(5, '-0.371*\"matrix\" + -0.268*\"np\" + 0.241*\"integr\" + 0.225*\"math\" + 0.214*\"seri\" + 0.196*\"coeffici\" + -0.168*\"plt\" + 0.166*\"fals\" + -0.161*\"window\" + -0.158*\"spars\"')\n",
      "(6, '-0.361*\"fals\" + -0.321*\"true\" + 0.270*\"axi\" + -0.257*\"math\" + -0.230*\"mask\" + -0.221*\"distanc\" + -0.200*\"window\" + 0.193*\"option\" + -0.189*\"plt\" + 0.177*\"default\"')\n",
      "(7, '-0.310*\"distanc\" + 0.229*\"coeffici\" + -0.222*\"option\" + -0.212*\"math\" + 0.205*\"seri\" + 0.195*\"np\" + 0.186*\"polynomi\" + 0.184*\"fit\" + -0.165*\"cluster\" + -0.162*\"comput\"')\n",
      "(8, '-0.359*\"matrix\" + -0.286*\"fals\" + 0.274*\"shape\" + 0.250*\"arrai\" + -0.236*\"true\" + -0.187*\"mask\" + 0.185*\"n_sampl\" + -0.184*\"option\" + 0.158*\"function\" + -0.158*\"dtype\"')\n",
      "(9, '-0.454*\"np\" + -0.254*\"dtype\" + 0.242*\"filter\" + -0.195*\"data\" + -0.162*\"type\" + -0.161*\"fit\" + 0.157*\"eigenvalu\" + 0.156*\"arrai\" + -0.141*\"float\" + 0.140*\"matrix\"')\n",
      "(10, '0.332*\"axi\" + 0.311*\"bin\" + 0.284*\"valu\" + -0.220*\"dtype\" + 0.200*\"norm\" + -0.198*\"type\" + 0.193*\"comput\" + -0.165*\"order\" + -0.164*\"filter\" + 0.157*\"ax\"')\n",
      "(11, '0.323*\"shape\" + 0.269*\"filter\" + 0.256*\"input\" + -0.249*\"integr\" + -0.225*\"window\" + -0.188*\"arrai\" + 0.186*\"ndarrai\" + -0.166*\"sampl\" + -0.156*\"eigenvalu\" + -0.151*\"label\"')\n",
      "(12, '0.465*\"bin\" + -0.377*\"axi\" + 0.246*\"valu\" + -0.206*\"shape\" + 0.169*\"filter\" + -0.159*\"n_sampl\" + 0.132*\"eigenvalu\" + 0.131*\"arrai\" + -0.117*\"fals\" + 0.115*\"histogram\"')\n",
      "(13, '-0.403*\"label\" + -0.254*\"averag\" + 0.233*\"shape\" + 0.224*\"integr\" + -0.203*\"filter\" + -0.175*\"input\" + -0.170*\"weight\" + 0.164*\"window\" + 0.151*\"bin\" + -0.142*\"score\"')\n",
      "(14, '-0.389*\"integr\" + -0.272*\"function\" + 0.228*\"option\" + -0.196*\"float\" + 0.181*\"cluster\" + 0.180*\"math\" + 0.166*\"default\" + 0.140*\"file\" + -0.138*\"label\" + -0.126*\"class\"')\n",
      "(15, '-0.319*\"np\" + -0.261*\"label\" + 0.260*\"type\" + 0.259*\"data\" + -0.226*\"input\" + 0.183*\"axi\" + 0.177*\"filter\" + -0.177*\"element\" + -0.151*\"true\" + 0.147*\"comput\"')\n",
      "(16, '-0.210*\"norm\" + 0.207*\"data\" + 0.201*\"object\" + -0.175*\"arrai\" + 0.165*\"function\" + -0.164*\"filter\" + 0.162*\"type\" + 0.159*\"matrix\" + -0.153*\"float\" + 0.151*\"paramet\"')\n",
      "(17, '0.300*\"int\" + 0.294*\"bin\" + -0.243*\"input\" + 0.234*\"cluster\" + 0.178*\"axi\" + 0.160*\"ndarrai\" + 0.149*\"shape\" + 0.142*\"math\" + -0.140*\"function\" + -0.134*\"data\"')\n",
      "(18, '-0.293*\"test\" + 0.270*\"window\" + -0.208*\"distribut\" + 0.198*\"int\" + -0.189*\"stat\" + -0.179*\"filter\" + 0.174*\"valu\" + 0.166*\"weight\" + -0.158*\"paramet\" + 0.155*\"fit\"')\n",
      "(19, '0.339*\"integr\" + 0.228*\"filter\" + 0.226*\"np\" + -0.211*\"window\" + -0.196*\"seri\" + 0.162*\"weight\" + 0.160*\"data\" + -0.158*\"root\" + 0.152*\"math\" + -0.140*\"comput\"')\n"
     ]
    }
   ],
   "source": [
    "# extract 400 LSI topics; use the default one-pass algorithm\n",
    "lsi = g.models.lsimodel.LsiModel(corpus=corpus, id2word=id2word, num_topics=n_topics)\n",
    "# print the most contributing words (both positively and negatively) for each of the first n_topic topics\n",
    "print(*lsi.print_topics(n_topics), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1130, 1.0), (1131, 0.99333835), (51, 0.93683767), (913, 0.91858041), (56, 0.89586836), (1117, 0.89080298), (1119, 0.88387483), (1110, 0.86899775), (90, 0.8591128), (952, 0.85390639)]\n"
     ]
    }
   ],
   "source": [
    "lll = lsi\n",
    "index = g.similarities.SparseMatrixSimilarity(lll[corpus], num_features=22)\n",
    "sims = index[lll[id2word.doc2bow(texts[1130])]]\n",
    "print(list(sorted(enumerate(sims), key=lambda t : t[1], reverse=True))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
